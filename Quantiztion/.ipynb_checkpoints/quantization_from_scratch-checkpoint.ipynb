{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6639ec6d-9d3a-4501-97b4-8498a598da91",
   "metadata": {},
   "source": [
    "## What is Quantization?\r\n",
    "\r\n",
    "**Quantization** is a technique in machine learning that involves reducing the precision of the numerical values used by a model. Typically, models are trained using **32-bit floating-point (FP32)** precision, but quantization reduces this to lower precision formats such as **16-bit floating-point (FP16)** or **8-bit integer (INT8)**. \r\n",
    "\r\n",
    "### Why is Quantization Done?\r\n",
    "\r\n",
    "Quantization is particularly valuable in situations where **memory** and **computational resources** are limited. The main benefits include:\r\n",
    "\r\n",
    "- **Reducing model size:** By lowering the precision of the parameters, the overall size of the model decreases, allowing it to fit into devices with limited storage, such as mobile phones or embedded systems.\r\n",
    "- **Speeding up inference:** Lower precision arithmetic is faster to compute, which leads to faster inference times, crucial for **real-time applications** and **low-latency systems**.\r\n",
    "- **Lower power consumption:** This is especially important in **edge devices** and **IoT (Internet of Things)** environments where battery life and energy efficiency are key constraints.\r\n",
    "- **Deploying on mobile and embedded devices:** Quantization enables large, complex models to be run on devices like **smartphones, tablets**, and **autonomous vehicles**, where full precision models would otherwise be too slow or resource-hungry.\r\n",
    "\r\n",
    "### Key Processes Involved in Quantization:\r\n",
    "\r\n",
    "1. **Training or Post-Training Quantization**: This can either be applied during the training phase, where the model learns to adapt to lower precision weights, or as a **post-training** step where the model is quantized after being fully trained.\r\n",
    "   \r\n",
    "2. **Quantization-Aware Training (QAT)**: This involves simulating the effects of quantization during training, so the model can compensate for the reduced precision and maintain higher accuracy.\r\n",
    "\r\n",
    "3. **Post-Training Quantization (PTQ)**: A simpler approach where a pre-trained model is quantized without re-training, typically used when computational resources for re-training are limited.\r\n",
    "\r\n",
    "4. **Dynamic and Static Quantization**: In **dynamic quantization**, certain parts of the model (like weights) are quantized only during inference, while **static quantization** involves quantizing both weights and activations beforehand.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "In this notebook, we'll explore how to implement quantization from scratch, examining the core steps involved in reducing precision while keeping the model's performance intact.\r\n",
    "racy.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c647e-1a39-4ed7-a301-245a5f23cdd6",
   "metadata": {},
   "source": [
    "## Simulating a Tensor with Random Values\r\n",
    "\r\n",
    "In this section, we generate a simple array of random numbers to simulate what a **tensor** might look like in practice. Using **NumPy**, we create an array with values drawn from a uniform distribution between **-20** and **200**. A few elements are manually adjusted to simulate specific scenarios, making debugging easier. \r\n",
    "\r\n",
    "This array gives us a basic structure to work with as we move forward in exploring quantization techniques.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4590e40d-4c76-450a-b2d7-9e1085bff7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.set_printoptions(suppress = True)\n",
    "\n",
    "# Generatinf random distribution \"tensors\"\n",
    "params = np.random.uniform(low = -20, high = 200, size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710c60bc-1f58-4d26-ba17-bdae5dea2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifing for easy debugging\n",
    "params[0] = params.max() + 1\n",
    "params[1] = params.min() - 1\n",
    "params[2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563958fd-0b4a-4eb6-9f30-144b55bcd521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([199.9341,  -9.1758,   0.    ,   7.1529, 123.2222, 101.2439,\n",
       "       182.7486,  88.2835,  58.6505,  -8.1758,  -4.9222, 159.3742,\n",
       "       110.0036, 159.674 , 198.9341])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = np.round(params,4)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508bec6f-29ac-4b33-9065-f94cfc33149d",
   "metadata": {},
   "source": [
    "## Clamping Values for Controlled Range\r\n",
    "\r\n",
    "One of the important steps in quantization is ensuring that the values in a tensor remain within a specified range. This is where **clamping** comes into play. Clamping restricts the values in an array to lie within defined **lower** and **upper bounds**.\r\n",
    "\r\n",
    "By applying clamping, any values below the lower bound will be set to the lower limit, and any values above the upper bound will be set to the upper limit. This helps in **normalizing** and **stabilizing** the data, making it more manageable for further quantization steps.\r\n",
    "\r\n",
    "The clamping function allows us to keep our tensor's values within a well-defined range, preparing the data for the quantization process.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2e91e68-51e5-44bf-9359-745c25481771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamping(params_arr, lower_bound, upper_bound):\n",
    "    params_arr[params_arr < lower_bound] = lower_bound\n",
    "    params_arr[params_arr > upper_bound] = upper_bound\n",
    "    return params_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a702c9e-72c7-4529-80c5-b75d5217b900",
   "metadata": {},
   "source": [
    "## Asymmetric Quantization and Dequantization\r\n",
    "\r\n",
    "Asymmetric quantization is a critical process in reducing the precision of tensor values while preserving the data's integrity. It involves mapping a floating-point range to a lower-bit integer range, which is particularly useful for deploying models on resource-constrained devices.\r\n",
    "\r\n",
    "### Asymmetric Quantization\r\n",
    "\r\n",
    "The quantization process can be defined as follows:\r\n",
    "\r\n",
    "1. **Calculate Scale and Zero Point**:\r\n",
    "   - **Alpha**: The maximum value in the tensor.\r\n",
    "   - **Beta**: The minimum value in the tensor.\r\n",
    "   - **Scale**: \r\n",
    "   $$\r\n",
    "   \\text{scale} = \\frac{\\alpha - \\beta}{2^{\\text{bits}} - 1}\r\n",
    "   $$\r\n",
    "   - **Zero Point**: \r\n",
    "   $$\r\n",
    "   \\text{zero\\_point} = -1 \\cdot \\text{round}\\left(\\frac{\\beta}{\\text{scale}}\\right)\r\n",
    "   $$\r\n",
    "\r\n",
    "2. **Define Lower and Upper Bounds**:\r\n",
    "   - The **lower bound** and **upper bound** for quantization are defined as:\r\n",
    "   $$\r\n",
    "   \\text{lower\\_bound} = 0\r\n",
    "   $$\r\n",
    "   $$\r\n",
    "   \\text{upper\\_bound} = 2^{\\text{bits}} - 1\r\n",
    "   $$\r\n",
    "   These bounds define the range of integer values that can be represented after quantization.\r\n",
    "\r\n",
    "3. **Quantization**:\r\n",
    "   - The original parameters are scaled and shifted using the calculated scale and zero point:\r\n",
    "   $$\r\n",
    "   \\text{quantized} = \\text{clamping}\\left(\\text{round}\\left(\\frac{\\text{params}}{\\text{scale}} + \\text{zero\\_point}\\right), \\text{lower\\_bound}, \\text{upper\\_bound}\\right)\r\n",
    "   $$\r\n",
    "\r\n",
    "### Asymmetric Dequantization\r\n",
    "\r\n",
    "The dequantization process retrieves the original floating-point values from the quantized integers using the reverse of the quantization formula:\r\n",
    "$$\r\n",
    "\\text{dequantized} = \\text{scale} \\times (\\text{params\\_q} - \\text{zero\\_point})\r\n",
    "$$\r\n",
    "\r\n",
    "These functions allow us to efficiently convert between floating-point and quantized representations, facilitating better performance and lower resource usage in machine learning models.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "136878cc-2db5-4bb6-a00e-bb52efd85f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetric_quantization(params, bits):\n",
    "    alpha = np.max(params) # the largest value in our \"tensor\"\n",
    "    beta = np.min(params) # smallest value in our \"tensor\"\n",
    "    scale = (alpha - beta) / (2**bits - 1) # here wa can also use min/max scaler\n",
    "    zero_point = -1*np.round(beta / scale)\n",
    "    lower_bound, upper_bound = 0, (2**bits - 1)\n",
    "    # Quantization \n",
    "    quantized = clamping(np.round(params/scale + zero_point), lower_bound, upper_bound).astype(np.int32)\n",
    "    return quantized, scale, zero_point\n",
    "\n",
    "def asymmetric_dequantization(params_q, scale, zero_point):\n",
    "    return scale * (params_q - zero_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ff6f3-99b0-4606-9c59-fbee3587460f",
   "metadata": {},
   "source": [
    "## Symmetric Quantization and Dequantization\r\n",
    "\r\n",
    "Symmetric quantization is a technique used to efficiently represent tensor values in a lower-bit integer format while ensuring that both positive and negative values are equally represented. This method is especially useful in scenarios where the tensor values are symmetrically distributed around zero.\r\n",
    "\r\n",
    "### Symmetric Quantization\r\n",
    "\r\n",
    "The quantization process can be defined as follows:\r\n",
    "\r\n",
    "1. **Calculate Scale**:\r\n",
    "   - **Alpha**: The maximum absolute value in the tensor:\r\n",
    "   $$\r\n",
    "   \\alpha = \\max(\\lvert \\text{params} \\rvert)\r\n",
    "   $$\r\n",
    "   - **Scale**: \r\n",
    "   $$\r\n",
    "   \\text{scale} = \\frac{\\alpha}{2^{(\\text{bits}-1)} - 1}\r\n",
    "   $$\r\n",
    "\r\n",
    "2. **Define Lower and Upper Bounds**:\r\n",
    "   - The **lower bound** and **upper bound** for quantization are defined as:\r\n",
    "   $$\r\n",
    "   \\text{lower\\_bound} = -2^{(\\text{bits} - 1)} - 1\r\n",
    "   $$\r\n",
    "   $$\r\n",
    "   \\text{upper\\_bound} = 2^{(\\text{bits} - 1)} - 1\r\n",
    "   $$\r\n",
    "   These bounds define the range of integer values that can be represented after quantization.\r\n",
    "\r\n",
    "3. **Quantization**:\r\n",
    "   - The original parameters are scaled and rounded using the calculated scale:\r\n",
    "   $$\r\n",
    "   \\text{quantized} = \\text{clamping}\\left(\\text{round}\\left(\\frac{\\text{params}}{\\text{scale}}\\right), \\text{lower\\_bound}, \\text{upper\\_bound}\\right)\r\n",
    "   $$\r\n",
    "\r\n",
    "### Symmetric Dequantization\r\n",
    "\r\n",
    "The dequantization process retrieves the original floating-point values from the quantized integers using the following formula:\r\n",
    "$$\r\n",
    "\\text{dequantized} = \\text{scale} \\times \\text{params\\_q}\r\n",
    "$$\r\n",
    "\r\n",
    "These functions allow us to efficiently convert between floating-point and quantized representations, enhancing model performance and reducing resource usage on various platforms.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c47142a8-abff-4868-885a-6251c85ad6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_quantization(params, bits):\n",
    "    alpha = np.max(np.abs(params)) # the max absolute value\n",
    "    scale = alpha / (2**(bits-1) - 1)\n",
    "    lower_bound, upper_bound = -2**(bits - 1) - 1, 2**(bits-1) - 1\n",
    "    # Quantization \n",
    "    quantized = clamping(np.round(params/scale), lower_bound, upper_bound).astype(np.int32)\n",
    "    return quantized, scale\n",
    "\n",
    "def symmetric_dequantization(params_q, scale):\n",
    "    return scale * params_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a876ca-7d82-4bc6-b99d-766463ba1706",
   "metadata": {},
   "source": [
    "## Quantization Error\r\n",
    "\r\n",
    "Quantization error refers to the **difference** between the original floating-point values of a tensor and their quantized representations. This error is crucial to assess as it significantly impacts the model's **accuracy** after quantization.\r\n",
    "\r\n",
    "In this case, we use **Mean Squared Error (MSE)** to quantify this error:\r\n",
    "\r\n",
    "- **MSE** helps us understand how much **information is lost** during the quantization process.\r\n",
    "- Minimizing this error is essential for maintaining the model's performance, especially when deploying to resource-constrained environments like **mobile devices** or **edge cases**.\r\n",
    "\r\n",
    "By evaluating quantization error, we can make informed decisions to enhance the effectiveness of the quantization process.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01e21fe5-5ebf-4274-ad53-d1e9428da9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error(params, params_q):\n",
    "    # we can calculate any for of loss here. ** MSE **\n",
    "    return np.mean((params - params_q)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211d40c-d67b-473d-af9e-2327e775c71c",
   "metadata": {},
   "source": [
    "## Seeing How It Plays Out\r\n",
    "\r\n",
    "In this section, we demonstrate the **quantization** process by converting floating-point values (FP) to integer (INT) representations using both **asymmetric** and **symmetric quantization** methods.\r\n",
    "\r\n",
    "### Asymmetric Quantization\r\n",
    "- **Calibration**: Floating-point values are scaled and shifted to fit within a specific integer range.\r\n",
    "- **Dequantization**: When converting back from INT to FP, we observe a slight difference between the original and dequantized values. This is due to the loss of precision inherent in the quantization process, especially with non-uniform data distributions.\r\n",
    "\r\n",
    "### Symmetric Quantization\r\n",
    "- **Calibration**: In this method, the data is scaled without a zero point, assuming a symmetric distribution around zero.\r\n",
    "- **Dequantization**: Similarly, converting back results in some loss of information. While the symmetric method is simpler, it can result in higher error when the data is not symmetrically distributed.\r\n",
    "\r\n",
    "### Observation\r\n",
    "Both methods result in reduced precision after dequantization, showcasing how quantization compresses data but slightly distorts the original values.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a259dc0c-0141-4c7d-ad57-aff9d8d81d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "[199.93  -9.18   0.     7.15 123.22 101.24 182.75  88.28  58.65  -8.18\n",
      "  -4.92 159.37 110.   159.67 198.93]\n",
      "\n",
      "Asymmetric scale: 0.8200388235294118, zero: 11.0\n",
      "[255   0  11  20 161 134 234 119  83   1   5 205 145 206 254]\n",
      "\n",
      "Symmetric scale: 1.5742842519685039\n",
      "[127  -6   0   5  78  64 116  56  37  -5  -3 101  70 101 126]\n"
     ]
    }
   ],
   "source": [
    "(asymmetric_q, asymmetric_scale, asymmetric_zero) = asymmetric_quantization(params, 8)\n",
    "(symmetric_q, symmetric_scale) = symmetric_quantization(params, 8)\n",
    "\n",
    "print(f'Original:')\n",
    "print(np.round(params, 2))\n",
    "print('')\n",
    "print(f'Asymmetric scale: {asymmetric_scale}, zero: {asymmetric_zero}')\n",
    "print(asymmetric_q)\n",
    "print('')\n",
    "print(f'Symmetric scale: {symmetric_scale}')\n",
    "print(symmetric_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "477df292-6986-4f72-923e-1d2dab21bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "[199.93  -9.18   0.     7.15 123.22 101.24 182.75  88.28  58.65  -8.18\n",
      "  -4.92 159.37 110.   159.67 198.93]\n",
      "\n",
      "Dequantize Asymmetric:\n",
      "[200.09  -9.02   0.     7.38 123.01 100.86 182.87  88.56  59.04  -8.2\n",
      "  -4.92 159.09 109.89 159.91 199.27]\n",
      "\n",
      "Dequantize Symmetric:\n",
      "[199.93  -9.45   0.     7.87 122.79 100.75 182.62  88.16  58.25  -7.87\n",
      "  -4.72 159.   110.2  159.   198.36]\n"
     ]
    }
   ],
   "source": [
    "# Dequantize the parameters back to 32 bits\n",
    "params_deq_asymmetric = asymmetric_dequantization(asymmetric_q, asymmetric_scale, asymmetric_zero)\n",
    "params_deq_symmetric = symmetric_dequantization(symmetric_q, symmetric_scale)\n",
    "\n",
    "print(f'Original:')\n",
    "print(np.round(params, 2))\n",
    "print('')\n",
    "print(f'Dequantize Asymmetric:')\n",
    "print(np.round(params_deq_asymmetric,2))\n",
    "print('')\n",
    "print(f'Dequantize Symmetric:')\n",
    "print(np.round(params_deq_symmetric, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27177ec3-aa4a-499e-8472-0e5bd0d75f68",
   "metadata": {},
   "source": [
    "### Error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "002ce1c0-cb47-46c5-81d3-e54eed6c2baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Asymmetric error: 0.05\n",
      "   Symmetric error: 0.15\n"
     ]
    }
   ],
   "source": [
    "# Calculate the quantization error\n",
    "print(f'{\"Asymmetric error: \":>20}{np.round(quantization_error(params, params_deq_asymmetric), 2)}')\n",
    "print(f'{\"Symmetric error: \":>20}{np.round(quantization_error(params, params_deq_symmetric), 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d107b22d-fc94-4c15-a7de-579c23f1c037",
   "metadata": {},
   "source": [
    "### Playing around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cb933be-64ec-48ff-83fe-e9dbfb04c6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symmetric tensor-like array:\n",
      "[-16.88845729 -14.73619101  -9.84795077  -2.16624371 -13.58845586\n",
      " -13.89295657  -5.13396663  -7.43403557  -2.02378329  -4.6139564\n",
      "  13.66297264  12.94906411   4.74121694   5.52796603   5.09095534\n",
      "   8.92738701   7.37523507   2.56579524  16.80063994  13.47113548]\n"
     ]
    }
   ],
   "source": [
    "# Generate 10 random positive values and 10 random negative values\n",
    "positive_values = np.random.uniform(low=0, high=20, size=10)\n",
    "negative_values = np.random.uniform(low=-20, high=0, size=10)\n",
    "\n",
    "# Concatenate the arrays to simulate symmetry\n",
    "params_sy = np.concatenate((negative_values, positive_values))\n",
    "\n",
    "# For an additional step, you can shuffle the values if needed\n",
    "# np.random.shuffle(params)\n",
    "\n",
    "print(\"Symmetric tensor-like array:\")\n",
    "print(params_sy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9b08034-1647-4fca-bdc8-05ebde8ad973",
   "metadata": {},
   "outputs": [],
   "source": [
    "(symmetric_q, symmetric_scale) = symmetric_quantization(params_sy, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc48dc4c-55ba-452b-a6ee-0feed550c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "[-16.89 -14.74  -9.85  -2.17 -13.59 -13.89  -5.13  -7.43  -2.02  -4.61\n",
      "  13.66  12.95   4.74   5.53   5.09   8.93   7.38   2.57  16.8   13.47]\n",
      "\n",
      "Symmetric scale: 0.13297997865189634\n",
      "[-127 -111  -74  -16 -102 -104  -39  -56  -15  -35  103   97   36   42\n",
      "   38   67   55   19  126  101]\n"
     ]
    }
   ],
   "source": [
    "print(f'Original:')\n",
    "print(np.round(params_sy, 2))\n",
    "print('')\n",
    "print(f'Symmetric scale: {symmetric_scale}')\n",
    "print(symmetric_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "789ee501-2e60-4b87-826b-fce0fb637722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "[-16.89 -14.74  -9.85  -2.17 -13.59 -13.89  -5.13  -7.43  -2.02  -4.61\n",
      "  13.66  12.95   4.74   5.53   5.09   8.93   7.38   2.57  16.8   13.47]\n",
      "\n",
      "Dequantize Symmetric:\n",
      "[-16.89 -14.76  -9.84  -2.13 -13.56 -13.83  -5.19  -7.45  -1.99  -4.65\n",
      "  13.7   12.9    4.79   5.59   5.05   8.91   7.31   2.53  16.76  13.43]\n"
     ]
    }
   ],
   "source": [
    "params_deq_symmetric = symmetric_dequantization(symmetric_q, symmetric_scale)\n",
    "print(f'Original:')\n",
    "print(np.round(params_sy, 2))\n",
    "print('')\n",
    "print(f'Dequantize Symmetric:')\n",
    "print(np.round(params_deq_symmetric, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f59ca02-61c8-40a5-bbf9-92e40f49a903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Symmetric error: 0.00159\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"Symmetric error: \":>20}{np.round(quantization_error(params_sy, params_deq_symmetric), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a55a0-7411-4458-b80f-98261397386d",
   "metadata": {},
   "source": [
    "#### Because of the nature of the data we can seehow the error is very low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4dda2b-cda1-4dab-92b2-d22e7b04653f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1510f-b1d2-4daa-9f42-8998b0f12b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183e31f-0f18-42a9-983f-7aa871768071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b4fa8-90b9-4408-9637-e1baa8e40fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
