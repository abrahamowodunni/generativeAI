{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f2e74a-bda8-49fb-96ca-056bbe32f5ce",
   "metadata": {},
   "source": [
    "# üåü Understanding Quantization Awareness Training\r\n",
    "\r\n",
    "Quantization Awareness Training (QAT) is an advanced technique aimed at improving the robustness of deep learning models during the quantization process. Unlike traditional post-training quantization, which involves converting a pre-trained model to lower precision after training, QAT integrates quantization into the training process itself. \r\n",
    "\r\n",
    "By simulating the effects of quantization during training, QAT allows the model to learn and adapt to the challenges posed by reduced precision, resulting in better performance and less accuracy degradation when deployed in quantized formats. \r\n",
    "\r\n",
    "## üîÑ Workflow of Quantization Awareness Training:\r\n",
    "\r\n",
    "**Pre-trained Model** ‚Üí **Quantization Simulation** ‚Üí **Training with Quantization** ‚Üí **Quantized Model**\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## üìñ Detailed Explanation:\r\n",
    "\r\n",
    "1. **Pre-trained Model:** \r\n",
    "   - This is the starting point, typically a model that has been trained using standard floating-point precision. The goal is to prepare this model for quantization through fine-tuning.\r\n",
    "\r\n",
    "2. **Quantization Simulation:** \r\n",
    "   - During this phase, the training process incorporates quantization effects by simulating lower precision (e.g., int8) operations. This is achieved by introducing quantization layers that mimic the behavior of quantized weights and activations.\r\n",
    "\r\n",
    "3. **Training with Quantization:** \r\n",
    "   - The model is trained with quantization-aware layers, enabling it to learn how to minimize the loss and maintain accuracy despite the constraints of reduced precision. This step is crucial for adapting the model to the quantization effects and ensures it remains effective when converted.\r\n",
    "\r\n",
    "4. **Quantized Model:** \r\n",
    "   - At the end of the training process, the model is transformed into a quantized version, optimized to perform well under quantization. The result is a model that can efficiently operate in low-resource environments while maintaining a higher accuracy compared to models trained without awareness of quantization.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "By implementing **Quantization Awareness Training**, practitioners can achieve more reliable quantized models, effectively balancing the trade-offs between efficiency and accuracy. üéØ\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a72c3fe-deac-45d5-abab-296cd14cd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e7b37-822d-49fa-97fb-77e2c6b474f1",
   "metadata": {},
   "source": [
    "### üìä Loading the Dataset\r\n",
    "\r\n",
    "In this section, we prepare the dataset that will be critical for training our neural network. We utilize the **MNIST dataset**, a well-known collection of handwritten digits that serves as a benchmark for image classification tasks.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üîÑ **Reproducibility**: We ensure consistent results across runs by setting a manual seed, which helps in achieving reproducible experiments.\r\n",
    "\r\n",
    "üñºÔ∏è **Data Transformations**: The dataset undergoes essential transformations:\r\n",
    "- **Tensor Conversion**: We convert images into tensor format to enable compatibility with PyTorch models.\r\n",
    "- **Normalization**: Normalizing the dataset is vital for enhancing training efficiency and improving overall model performance.\r\n",
    "\r\n",
    "üöÄ **Data Loaders**: We create data loaders for both the training and testing datasets, which facilitate efficient mini-batch processing:\r\n",
    "- **Mini-batch Processing**: This technique allows our model to learn from smaller subsets of data, speeding up training and enhancing generalization.\r\n",
    "\r\n",
    "üíª **GPU Utilization**: Lastly, we configure our environment to leverage GPU resources, optimizing computational efficiency when available.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "This foundational step prepares us for the subsequent stages of model training and quantization, enabling us to effectively implement Quantization Aware Training on our model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843966e6-12f3-4632-8348-432ae16c0e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(433)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b9c2af-6779-4af4-a647-5683232c1e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # converting to tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # performing normalization on the data which is optimal in ML or DL\n",
    "])\n",
    "\n",
    "# we would be using the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# creating batch norm\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# trying to leverage my baby GPU hahahaha ;)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef2a53d-3729-4c85-b6c6-33a88f6a2a27",
   "metadata": {},
   "source": [
    "### üß† Simple Neural Network\r\n",
    "\r\n",
    "In this section, we define a simple neural network that incorporates **Quantization Aware Training (QAT)**. This approach allows us to account for quantization effects during the training process, thereby improving the model's performance in a quantized state.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üîç **Network Architecture**: The network comprises several layers:\r\n",
    "- **Input Layer**: Flattens the input images (28x28) into a 784-dimensional vector.\r\n",
    "- **Hidden Layers**: \r\n",
    "  - **First Hidden Layer**: 50 neurons\r\n",
    "  - **Second Hidden Layer**: 80 neurons\r\n",
    "  - **Third Hidden Layer**: 30 neurons\r\n",
    "- **Output Layer**: 10 neurons corresponding to the digit classes (0-9).\r\n",
    "\r\n",
    "üì¶ **Quantization Stubs**: \r\n",
    "- **QuantStub**: This layer simulates the quantization of the input data.\r\n",
    "- **DeQuantStub**: This layer reverses the quantization after processing through the network.\r\n",
    "\r\n",
    "üí° **Forward Pass**: The forward method includes:\r\n",
    "1. **Flattening**: Reshaping the input image to a vector.\r\n",
    "2. **Quantization**: Applying the `QuantStub` to the input.\r\n",
    "3. **Activation Functions**: Using ReLU activation after each hidden layer to introduce non-linearity.\r\n",
    "4. **Final Output**: Applying the `DeQuantStub` before returning the output.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Key Concept:\r\n",
    "Unlike Post-Training Quantization (PTQ), where weights are copied and quantized after training, **QAT** involves adding fake quantization layers to the model during training. This allows the model to learn how to compensate for the effects of quantization, resulting in better accuracy once deployed.\r\n",
    "\r\n",
    "This structure sets the stage for effectively implementing QAT and ensuring that our model retains performance even in its quantized form.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4683859-76e2-4c01-8720-ba59d04cf4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_layer_1 = 50,hidden_layer_2 = 80, hidden_layer_3 = 30):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_layer_1)\n",
    "        self.linear2 = nn.Linear(hidden_layer_1, hidden_layer_2)\n",
    "        self.linear3 = nn.Linear(hidden_layer_2, hidden_layer_3)\n",
    "        self.linear4 = nn.Linear(hidden_layer_3, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        \n",
    "    def forward(self,img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.quant(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e13d5d-c9fb-4b38-850a-c689c5977508",
   "metadata": {},
   "source": [
    "On like the case of PTQ, after training the weights are copied and then quantized based of the trained weight, in QAT we add fake quantized layers and then train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d29d696-e285-4169-9df2-ad0282731ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=50, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=50, out_features=80, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=80, out_features=30, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (linear4): Linear(\n",
       "    in_features=30, out_features=10, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.qconfig = torch.ao.quantization.default_qconfig\n",
    "model.train() # we aren't doing inferencing\n",
    "model_qat = torch.ao.quantization.prepare_qat(model) # Insert observers\n",
    "model_qat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3293c46c-c0e0-40ac-bbec-54166974bc7f",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Model Training\r\n",
    "\r\n",
    "In this section, we focus on training our neural network using the **MNIST dataset**. The training process is essential for the model to learn and improve its performance on the classification task.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üõ†Ô∏è **Components of Training**:\r\n",
    "1. **Optimizer**: We utilize the **Adam** optimizer, known for its efficiency in adjusting learning rates during training.\r\n",
    "2. **Loss Function**: The **Cross-Entropy Loss** is employed, suitable for multi-class classification problems like digit recognition.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üïí **Training Loop**:\r\n",
    "- **Epochs**: The model is trained for a specified number of epochs, allowing it to learn from the data iteratively.\r\n",
    "- **Total Iterations**: We keep track of the total iterations across epochs to manage training duration and avoid overfitting.\r\n",
    "\r\n",
    "### Training Steps:\r\n",
    "1. **Model in Training Mode**: The model is set to training mode to enable features like dropout and batch normalization.\r\n",
    "2. **Loss Calculation**: For each batch of data:\r\n",
    "   - Inputs (images) and targets (labels) are moved to the appropriate device (CPU/GPU).\r\n",
    "   - The model outputs predictions for the input data.\r\n",
    "   - The loss is calculated using the defined loss function.\r\n",
    "3. **Backpropagation**: The optimizer updates the model parameters based on the computed gradients to minimize the loss.\r\n",
    "4. **Progress Tracking**: The average loss is updated and displayed, providing insight into the model's learning process.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üí° **Iteration Limit**: An optional limit on total iterations can be set, enabling early stopping if desired, thus preventing unnecessary training and resource usage.\r\n",
    "\r\n",
    "This structured training approach is crucial for optimizing our neural network and preparing it for effective **Quantization Aware Training (QAT)**.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15e93b8-7afc-448e-8b9a-cf67106e61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, epochs = None, total_iterations_limit = None):\n",
    "    # optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss() # since this is a classification problem.\n",
    "\n",
    "    total_iterations = 0  # Keep track of how many total iterations we've done\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        loss_sum = 0  # Sum of all the losses to calculate the average loss\n",
    "        num_iterations = 0  # Keep track of the iterations in this epoch\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data # 'data' is a batch (x, y), where x is the input (image), and y is the label (digit)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x.view(-1, 28*28))\n",
    "            loss = loss_function(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # If a total iteration limit is set, stop training once the limit is reached\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96a218-d8b8-45eb-9c66-eb723d3361dd",
   "metadata": {},
   "source": [
    "### üìè Function to Print the Size of the Model\r\n",
    "\r\n",
    "Understanding the size of our model is crucial for evaluating its deployment feasibility, especially in resource-constrained environments. This section introduces a function that measures the size of the model after training.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üìù **Function Overview**:\r\n",
    "- **Model Size Calculation**: The function saves the model‚Äôs state dictionary temporarily to compute its file size. This provides a direct indication of the model's memory footprint in kilobytes (KB).\r\n",
    "\r\n",
    "#### Key Steps:\r\n",
    "1. **Saving Model State**: The model's state dictionary is saved to a temporary file.\r\n",
    "2. **Size Retrieval**: The file size is retrieved and printed in kilobytes for clarity.\r\n",
    "3. **Cleanup**: The temporary file is deleted to maintain a clean workspace.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üìÇ **Model Loading**:\r\n",
    "- If a pre-trained model exists, it is loaded from disk, ensuring we can resume from previous training without starting from scratch.\r\n",
    "- If not found, the model undergoes training for a specified number of epochs, followed by saving the newly trained model.\r\n",
    "\r\n",
    "#### Example Output:\r\n",
    "During training, the model's performance can be tracked, showcasing the loss over iterations. For instance, you may observe a progress bar indicating the completion of an epoch, reflecting the loss value.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "This process not only facilitates model evaluation but also prepares us for the next steps in **Quantization Aware Training (QAT)**, ensuring we can optimize the model for efficient deployment.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "471ecaff-9a68-4c13-bc64-ee55bb076d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [00:38<00:00, 157.61it/s, loss=0.263]\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp_delme.p\")\n",
    "    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n",
    "    os.remove('temp_delme.p')\n",
    "\n",
    "MODEL_FILENAME = 'simpleNN_qat.pt'\n",
    "\n",
    "if Path(MODEL_FILENAME).exists():\n",
    "    model.load_state_dict(torch.load(MODEL_FILENAME))\n",
    "    print('Loaded model from disk')\n",
    "else:\n",
    "    train(train_loader, model_qat, epochs=1)\n",
    "    # Save the model to disk\n",
    "    torch.save(model.state_dict(), MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7ce0a-2874-4aac-abf2-d9d88359bcf7",
   "metadata": {},
   "source": [
    "### üß™ Time to Test Our Neural Network\r\n",
    "\r\n",
    "After training our model, it's essential to evaluate its performance using a testing phase. This section outlines the process of testing our neural network and calculating its accuracy.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "üîç **Testing Overview**:\r\n",
    "- **Model Evaluation**: We switch the model to evaluation mode, which is crucial as it disables layers like dropout that are only used during training.\r\n",
    "- **No Gradient Calculation**: Using `torch.no_grad()` ensures that we don't compute gradients, thus saving memory and computation time during testing.\r\n",
    "\r\n",
    "#### Testing Steps:\r\n",
    "1. **Initialization**: Set counters for correct predictions and total predictions.\r\n",
    "2. **Data Iteration**: Iterate over the test dataset using a progress bar to visualize progress.\r\n",
    "3. **Model Prediction**:\r\n",
    "   - Input test images into the model.\r\n",
    "   - Calculate the predicted outputs.\r\n",
    "4. **Accuracy Calculation**:\r\n",
    "   - Compare predicted labels with actual labels to count correct predictions.\r\n",
    "   - Update total predictions accordingly.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "‚úÖ **Final Accuracy**: At the end of the testing phase, the accuracy is printed, providing insight into how well our model has generalized from the training data.\r\n",
    "\r\n",
    "#### Example Output:\r\n",
    "Upon completion of the testing process, you will see the accuracy printed in a user-friendly format, indicating the model's performance.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "This testing phase is crucial for understanding the model's effectiveness and readiness for deployment, paving the way for further optimizations like quantization.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e517cd11-e7f1-46c5-9aed-8f69e7106199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, total_iterations):\n",
    "    correct,total, iterations = 0,0,0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x.view(-1, 784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                total +=1\n",
    "            iterations += 1\n",
    "            if total_iterations is not None and iterations >= total_iterations:\n",
    "                break\n",
    "    print(f'Accuracy: {round(correct/total, 8)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e3fea-3edf-465e-b687-53d1f1938284",
   "metadata": {},
   "source": [
    "### üìä Awareness Statistics\n",
    "\n",
    "In this section, we delve into the statistics of the various layers within our neural network model. Understanding these statistics is crucial for monitoring the performance and effectiveness of our quantization-aware training (QAT).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d882e12-69f9-4c16-960b-5e0bdf63c3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n",
       "  )\n",
       "  (linear1): Linear(\n",
       "    in_features=784, out_features=50, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.5536828637123108, max_val=0.34701329469680786)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-43.13856506347656, max_val=33.60527420043945)\n",
       "  )\n",
       "  (linear2): Linear(\n",
       "    in_features=50, out_features=80, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.43600958585739136, max_val=0.4428870975971222)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-31.863323211669922, max_val=27.115581512451172)\n",
       "  )\n",
       "  (linear3): Linear(\n",
       "    in_features=80, out_features=30, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.3675735592842102, max_val=0.3865065574645996)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-20.646923065185547, max_val=30.78984832763672)\n",
       "  )\n",
       "  (linear4): Linear(\n",
       "    in_features=30, out_features=10, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.4331170916557312, max_val=0.28388720750808716)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-23.716779708862305, max_val=17.866180419921875)\n",
       "  )\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Check statistics of the various layers')\n",
    "model_qat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0180d602-9287-4485-9416-a6f219edd6f9",
   "metadata": {},
   "source": [
    "---\r\n",
    "\r\n",
    "üîç **Layer Statistics Overview**:\r\n",
    "- Each layer's statistics provide insights into the distribution of activations and weights, essential for evaluating the impact of quantization.\r\n",
    "- We use **MinMaxObserver** to track the minimum and maximum values of activations and weights across layers.\r\n",
    "\r\n",
    "#### Layer Breakdown:\r\n",
    "- **Quantization Layer**:\r\n",
    "  - **QuantStub**: Prepares inputs for quantization and captures activation statistics.\r\n",
    "  \r\n",
    "- **Linear Layers**:\r\n",
    "  - Each linear layer (`linear1`, `linear2`, `linear3`, `linear4`) reports:\r\n",
    "    - **Input Features**: The number of features each layer receives.\r\n",
    "    - **Output Features**: The number of features each layer produces.\r\n",
    "    - **Weight Fake Quantization**: Minimum and maximum observed values to understand the scale of weights.\r\n",
    "    - **Activation Post-Process**: Minimum and maximum values for the activations after applying the activation function.\r\n",
    "\r\n",
    "- **Activation Function**:\r\n",
    "  - **ReLU**: Applies the rectified linear unit activation, which is widely used for hidden layers.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "‚úÖ **Model Structure**: The printed model summary provides a comprehensive overview of the layer statistics, helping identify any potential issues that could arise from quantization.\r\n",
    "\r\n",
    "#### Example Output:\r\n",
    "You can observe the statistical summary of each layer printed below, detailing the behavior of the model's components post-training.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "Understanding these statistics aids in fine-tuning our model further, ensuring that it maintains performance after quantization.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c9bf90-7585-44ef-ad34-a6d524c8439f",
   "metadata": {},
   "source": [
    "### üîç Quantization of the Model\n",
    "\n",
    "With our quantization-aware training (QAT) complete, it's time to convert the model to a quantized version using the statistics gathered during training. This process optimizes the model for efficiency without significantly sacrificing accuracy.\n",
    "\n",
    "#### **Quantization Process:**\n",
    "- The model is evaluated and converted, which adjusts the weights and activations based on the quantization parameters derived during training.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d1d71c-6235-412e-ae43-d573745dda14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check statistics of the various layers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n",
       "  (linear1): QuantizedLinear(in_features=784, out_features=50, scale=0.6042821407318115, zero_point=71, qscheme=torch.per_tensor_affine)\n",
       "  (linear2): QuantizedLinear(in_features=50, out_features=80, scale=0.4644008278846741, zero_point=69, qscheme=torch.per_tensor_affine)\n",
       "  (linear3): QuantizedLinear(in_features=80, out_features=30, scale=0.40501394867897034, zero_point=51, qscheme=torch.per_tensor_affine)\n",
       "  (linear4): QuantizedLinear(in_features=30, out_features=10, scale=0.32742488384246826, zero_point=72, qscheme=torch.per_tensor_affine)\n",
       "  (relu): ReLU()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qat.eval()\n",
    "model_qat = torch.ao.quantization.convert(model_qat)\n",
    "\n",
    "print(f'Check statistics of the various layers')\n",
    "model_qat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8beb5cc9-15cf-4b57-9f19-83eae5f3c4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights before quantization\n",
      "tensor([[ -3,  -4,  -9,  ...,  -8,   0,  -8],\n",
      "        [  1,  10,   5,  ...,  -1,   6,  -3],\n",
      "        [  5,  -7,  -5,  ..., -10,  -7,  -2],\n",
      "        ...,\n",
      "        [  1,   7,   5,  ...,   4,   2,   6],\n",
      "        [ -4,   7,   0,  ...,   4,   3,   5],\n",
      "        [ -5,   7,   2,  ...,   4,   3,   6]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "# Print the weights matrix of the model before quantization\n",
    "print('Weights before quantization')\n",
    "print(torch.int_repr(model_qat.linear1.weight()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e6bd4e7-d01f-4569-bd9e-7325dde1592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model after quantization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02<00:00, 392.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('Testing the model after quantization')\n",
    "test(model_qat,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4f80058-1e34-4dd3-af52-cf583c64ca5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the model after quantization\n",
      "Size (KB): 52.77\n"
     ]
    }
   ],
   "source": [
    "print('Size of the model after quantization')\n",
    "print_size_of_model(model_qat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4402b8-61cc-4d3f-924b-a177edca6632",
   "metadata": {},
   "source": [
    "\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "**Layer Statistics Post-Quantization:**\r\n",
    "After conversion, we check the statistics of the various layers in our quantized model:\r\n",
    "\r\n",
    "- **Quantization Parameters**:\r\n",
    "  - Each layer now has a scale and zero point, allowing for efficient computation with reduced precision.\r\n",
    "\r\n",
    "#### **Model Summary**:\r\n",
    "The model structure includes:\r\n",
    "- Quantized layers with `QuantizedLinear`, displaying input and output features along with their respective scales and zero points.\r\n",
    "- The activation function remains the same with ReLU.\r\n",
    "- DeQuantize layer for converting the quantized outputs back to floating-point format when needed.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "#### **Model Weights Before Quantization:**\r\n",
    "To understand the transformation, we also print the weights of the first linear layer before quantization, showcasing their original representation.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ‚úÖ **Model Performance After Quantization**:\r\n",
    "After quantization, we tested the model to evaluate its accuracy on the test dataset. Remarkably, the model achieved an accuracy of **95.02%**!\r\n",
    "\r\n",
    "#### **Model Size After Quantization**:\r\n",
    "Finally, we assess the size of the quantized model, which significantly reduces from its original size to approximately **52.77 KB**. This reduction in model size is beneficial for deployment in resource-constrained environments.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### üéâ Conclusion\r\n",
    "The quantization process successfully optimized the model, maintaining high accuracy while reducing its size, demonstrating the efficacy of QAT in deep learning model deployment.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d76a96b-b797-4f00-a5d5-4fd22ab7af74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
