{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24a1622b-b8e3-4ecf-bb6c-f3f1a28d6676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec853629-11f3-4c68-a277-7c596c89ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff89917-6940-4b59-aef0-b623f18f1f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0b1bf6-e641-4605-97b2-de254c36bef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'pdfs\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'page': 0}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b156e7-d527-4e62-8d2f-2ae48adba99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                                chunk_overlap=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e97094e9-d700-4a4d-aa4e-934308c95efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_splitter.split_documents(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0957566-5706-45ca-9430-d5bc2ba75fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\n",
      "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\n",
      "translation system: Bridging the gap between human and machine translation. arXiv preprint\n",
      "arXiv:1609.08144 , 2016.\n",
      "[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\n",
      "fast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(text_chunks[75].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd65c65-d305-4e05-925a-e19843664fc7",
   "metadata": {},
   "source": [
    "### Creating the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3f6ce8d-a5a7-40b8-ab16-d7b5db982ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92b1ef45-5141-4aee-885c-022a020d8ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'db'\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=text_chunks,\n",
    "                                embedding=embedding,\n",
    "                                persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2cdf7aa-5dd0-4a01-9d88-99d2cb4cb415",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist() # persist to your disk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6304e508-2e1f-4f0f-a243-131c0040baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddd27a72-113b-4113-a9ca-1d08e8c31bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d31f8396-71b8-4c67-afac-c056da91479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(persist_directory = persist_directory,\n",
    "                 embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6f0084-49ec-42b8-920a-3d174b46236f",
   "metadata": {},
   "source": [
    "### Make retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e45d0e88-24be-4669-b9a2-453b92b9923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "044ec379-2d15-4ce9-80d3-afe2153bd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriver.get_relevant_documents(\"How many attention heads was using in this paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fe13ca1-71bd-44c1-915e-999011d6c2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b45f5a53-bfd1-4638-b948-b052fef00eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'page': 4, 'source': 'pdfs\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf'}, page_content='MultiHead( Q,K,V ) = Concat(head 1,...,head h)WO\\nwhere head i= Attention( QWQ\\ni,KWK\\ni,VWV\\ni)\\nWhere the projections are parameter matrices WQ\\ni∈Rdmodel×dk,WK\\ni∈Rdmodel×dk,WV\\ni∈Rdmodel×dv\\nandWO∈Rhdv×dmodel.\\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06a4adce-db68-4322-ad91-d2d747154991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k': 2}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrive = vectordb.as_retriever(search_kwargs = {'k':2})\n",
    "retrive.search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c201fb6-9aa2-4559-b341-5c3b0d187f80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = retrive.get_relevant_documents(\"How many attention heads was using in this paper?\")\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30e95a35-a4fb-4409-ac44-5ad9ee473392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 7, 'source': 'pdfs\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf'}, page_content='development set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.'),\n",
       " Document(metadata={'page': 10, 'source': 'pdfs\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb096c4-092d-45c1-a796-5ac85800007d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2230dcc4-529f-4fd5-bbe5-168efbca6490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db4afb-8176-472e-ac16-730fd759e017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10407cd4-ded6-4d90-b037-a430bdedc3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa29ca4-a43b-4e39-92df-5f461ddbf369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d8fd8-4904-4b82-a006-15f2bbb0a707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af93f2-aee9-478b-aa2a-011420829403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed7f291-dee3-4419-a6cd-89296881e3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038b58e-eade-4ccc-8273-3b2269fd8741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e492b-c1b3-4c36-93b5-9232bc2c7be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba643b-0479-467b-92f9-c9fa8c8f3f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1752444-4e49-4211-bb77-2ba91b779c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea886c8-25ca-4b83-a057-478612368d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17d54c-50e8-4b26-ba3e-20540c6874c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e957f5-b83a-456b-9830-340033d74909",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7666ba-7541-45ae-bad0-ede27aa79f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1dea8d-f113-4716-a97c-4c5ce8bcb00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df2f2c-62bd-4cdd-9b91-ca5237799b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476195d-5e0e-4d78-b0f7-05b4d1a39e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a831066-cfbd-40f7-a749-7965de0245e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7211d1-e90a-4f00-b534-a2fc35eb27a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee64d29-81c1-4136-8f12-04ab949dfac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
