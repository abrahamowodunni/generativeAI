{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08aab63-9a8f-4271-97a9-8a8181cdf5dd",
   "metadata": {},
   "source": [
    "# üìä Overview of the Implementation of LoRA on an ANN\r\n",
    "\r\n",
    "In this notebook, we will explore the implementation of **Low-Rank Adaptation (LoRA)** on an Artificial Neural Network (ANN). LoRA is a technique designed to enhance the efficiency of model training by introducing low-rank parameterization, allowing for effective fine-tuning with fewer parameters.\r\n",
    "\r\n",
    "## üîç Objectives:\r\n",
    "- **Understand LoRA**: Grasp the fundamental concepts behind Low-Rank Adaptation and its advantages in parameter-efficient training.\r\n",
    "- **Setup the ANN**: Implement a basic ANN structure that we will adapt using LoRA.\r\n",
    "- **Parameterization**: Learn how to parameterize the weights of the network to include LoRA.\r\n",
    "- **Training**: Train the modified network and observe the impact on performance with reduced parameters.\r\n",
    "- **Analysis**: Compare the performance and parameters of the original network versus the LoRA-adapted network.\r\n",
    "\r\n",
    "## üöÄ Let's dive in and implement LoRA to make our ANN more efficient!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab68d39-4e45-41ed-a71f-84f9c9fb489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643b243-ba0f-4e42-9bf3-af55f0aa3451",
   "metadata": {},
   "source": [
    "### üì• Loading the Dataset\r\n",
    "\r\n",
    "In this section, we will load the **MNIST dataset**, a widely used dataset for training image processing systems. It consists of handwritten digits from 0 to 9, which we will use to train our ANN with the LoRA technique.\r\n",
    "\r\n",
    "#### üîÑ Data Transformations:\r\n",
    "- **Tensor Conversion**: The images will be converted to tensors, which are essential for operations in PyTorch.\r\n",
    "- **Normalization**: Each image will be normalized using the optimal mean and standard deviation values for better performance in machine learning and deep learning models.\r\n",
    "\r\n",
    "#### üîÑ Data Loaders:\r\n",
    "- We create **data loaders** for both the training and test sets, allowing us to efficiently batch and shuffle our data during training. Each batch will contain 10 images.\r\n",
    "\r\n",
    "#### üöÄ Leveraging GPU:\r\n",
    "- We check if a GPU is available and set our device accordingly. Using a GPU will significantly speed up the training process, making it easier to work with larger models and datasets.\r\n",
    "\r\n",
    "### Let's prepare the data for training our model with LoRA!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66fce46-3f88-4690-b400-58579f2ca00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # converting to tensors\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # performing normalization on the data which is optimal in ML or DL\n",
    "])\n",
    "\n",
    "# we would be using the MNIST dataset\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# creating batch norm\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)\n",
    "\n",
    "# trying to leverage my baby GPU hahahaha ;)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f67ba2-83d5-433d-a83c-1ae8b47cceb2",
   "metadata": {},
   "source": [
    "### üß† \"Complex\" ANN for This Task\r\n",
    "\r\n",
    "In this section, we define a **Complex Artificial Neural Network (ANN)** designed to handle the MNIST digit classification task. This architecture consists of multiple layers, allowing it to learn intricate patterns in the data.\r\n",
    "\r\n",
    "#### üîç Network Architecture:\r\n",
    "- **Input Layer**: The network takes images of size \\(28 \\times 28\\) pixels, flattened into a vector of size 784.\r\n",
    "- **Hidden Layers**:\r\n",
    "  - **First Hidden Layer**: 1000 neurons\r\n",
    "  - **Second Hidden Layer**: 1500 neurons\r\n",
    "  - **Third Hidden Layer**: 1300 neurons\r\n",
    "- **Output Layer**: The final layer outputs predictions for 10 classes, corresponding to the digits 0 through 9.\r\n",
    "\r\n",
    "#### ‚öôÔ∏è Activation Function:\r\n",
    "- **ReLU Activation**: The Rectified Linear Unit (ReLU) is applied after each hidden layer to introduce non-linearity, helping the model to learn complex relationships in the data.\r\n",
    "\r\n",
    "### üöÄ Moving to Device:\r\n",
    "- The model is transferred to the specified device (GPU or CPU) for efficient computation.\r\n",
    "\r\n",
    "With this architecture, we aim to leverage the power of deep learning to accurately classify handwritten digits from the MNIST dataset!\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002f756f-65ba-425a-bbcb-61780a3d412c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, hidden_layer_1 = 1000,hidden_layer_2 = 1500, hidden_layer_3 = 1300):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, hidden_layer_1)\n",
    "        self.linear2 = nn.Linear(hidden_layer_1, hidden_layer_2)\n",
    "        self.linear3 = nn.Linear(hidden_layer_2, hidden_layer_3)\n",
    "        self.linear4 = nn.Linear(hidden_layer_3, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,img):\n",
    "        x = img.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.relu(self.linear3(x))\n",
    "        x = self.linear4(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12954800-d644-437b-b9d1-c264bfa09afb",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Model Training\r\n",
    "\r\n",
    "In this section, we implement the training routine for our Complex ANN using the **Adam** optimizer and **Cross Entropy Loss**. The training process will involve iterating over the training dataset, updating the model parameters, and monitoring the loss.\r\n",
    "\r\n",
    "#### üîë Key Components:\r\n",
    "\r\n",
    "- **Optimizer**: \r\n",
    "  - **Adam**: A popular optimization algorithm that adapts the learning rate for each parameter, improving convergence speed.\r\n",
    "  \r\n",
    "- **Loss Function**: \r\n",
    "  - **Cross Entropy Loss**: Suitable for multi-class classification problems, it measures the performance of the model by comparing the predicted class probabilities with the true labels.\r\n",
    "\r\n",
    "#### üìà Training Loop:\r\n",
    "1. **Epochs**: The model is trained for a specified number of epochs.\r\n",
    "2. **Data Loader**: The training data is loaded in batches for efficient processing.\r\n",
    "3. **Forward Pass**:\r\n",
    "   - The input images are reshaped and passed through the model to obtain predictions.\r\n",
    "4. **Loss Calculation**:\r\n",
    "   - The loss is computed by comparing the model‚Äôs predictions with the actual labels.\r\n",
    "5. **Backward Pass**:\r\n",
    "   - The gradients are calculated through backpropagation, and the optimizer updates the model parameters accordingly.\r\n",
    "6. **Monitoring**:\r\n",
    "   - The average loss is calculated and displayed in real-time using a progress bar.\r\n",
    "\r\n",
    "This training routine allows the model to learn from the data effectively, improving its performance on the MNIST classification task. \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9001f6d9-4ac6-4c55-815a-3fb9e148e8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6000/6000 [07:15<00:00, 13.77it/s, loss=0.277]\n"
     ]
    }
   ],
   "source": [
    "def train(train_loader, model, epochs = None, total_iterations_limit = None):\n",
    "    # optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_function = nn.CrossEntropyLoss() # since this is a classification problem.\n",
    "\n",
    "    total_iterations = 0  # Keep track of how many total iterations we've done\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        loss_sum = 0  # Sum of all the losses to calculate the average loss\n",
    "        num_iterations = 0  # Keep track of the iterations in this epoch\n",
    "        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n",
    "\n",
    "        if total_iterations_limit is not None:\n",
    "            data_iterator.total = total_iterations_limit\n",
    "        for data in data_iterator:\n",
    "            num_iterations += 1\n",
    "            total_iterations += 1\n",
    "            x, y = data # 'data' is a batch (x, y), where x is the input (image), and y is the label (digit)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x.view(-1, 28*28))\n",
    "            loss = loss_function(output, y)\n",
    "            loss_sum += loss.item()\n",
    "            avg_loss = loss_sum / num_iterations\n",
    "            data_iterator.set_postfix(loss=avg_loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # If a total iteration limit is set, stop training once the limit is reached\n",
    "            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n",
    "                return\n",
    "\n",
    "train(train_loader, model, epochs= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be0d842-7ec7-4b36-80f6-96a57d734b40",
   "metadata": {},
   "source": [
    "### We keep the original weight of the model befor fine-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8105e1-ba64-482b-914d-0e0b7f388ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_weights = {}\n",
    "for name, param in model.named_parameters():\n",
    "    o_weights[name] = param.clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ba174-38b9-4e87-b8f5-a49d9e260321",
   "metadata": {},
   "source": [
    "### üìä Model Evaluation\r\n",
    "\r\n",
    "After training the model, it is crucial to evaluate its performance on the test dataset to understand its effectiveness in classifying the MNIST digits. In this section, we will implement a testing function to calculate accuracy and analyze misclassifications.\r\n",
    "\r\n",
    "#### üîë Evaluation Objectives:\r\n",
    "1. **Accuracy Calculation**:\r\n",
    "   - Determine the percentage of correctly classified instances out of the total number of test samples.\r\n",
    "   \r\n",
    "2. **Misclassification Analysis**:\r\n",
    "   - Count and report the number of incorrect predictions for each digit (0-9) to identify specific weaknesses in the model.\r\n",
    "\r\n",
    "#### üß™ Testing Function:\r\n",
    "- **No Gradient Tracking**: We utilize `torch.no_grad()` to disable gradient calculation, which reduces memory usage and speeds up computation during testing.\r\n",
    "- **Data Iteration**: The test data is iterated over in batches, similar to training.\r\n",
    "- **Output Comparison**:\r\n",
    "  - The predicted digit is compared to the actual label to count correct and incorrect classifications.\r\n",
    "\r\n",
    "#### üìà Performance Metrics:\r\n",
    "- **Accuracy**: Calculated as the ratio of correct predictions to total predictions.\r\n",
    "- **Wrong Counts**: A detailed breakdown of the number of misclassifications for each digit.\r\n",
    "\r\n",
    "This evaluation will provide insights into the model's strengths and areas for improvement, guiding future enhancements and refinements.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff5ab98-b434-4c5a-84d7-435352c6168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:05<00:00, 176.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.948\n",
      "wrong counts for the digit 0: 18\n",
      "wrong counts for the digit 1: 19\n",
      "wrong counts for the digit 2: 30\n",
      "wrong counts for the digit 3: 81\n",
      "wrong counts for the digit 4: 42\n",
      "wrong counts for the digit 5: 11\n",
      "wrong counts for the digit 6: 60\n",
      "wrong counts for the digit 7: 30\n",
      "wrong counts for the digit 8: 150\n",
      "wrong counts for the digit 9: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    wrong_counts = [0 for i in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader, desc='Testing'):\n",
    "            x, y = data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            output = model(x.view(-1, 784))\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y[idx]:\n",
    "                    correct +=1\n",
    "                else:\n",
    "                    wrong_counts[y[idx]] +=1\n",
    "                total +=1\n",
    "    print(f'Accuracy: {round(correct/total, 3)}')\n",
    "    for i in range(len(wrong_counts)):\n",
    "        print(f'wrong counts for the digit {i}: {wrong_counts[i]}')\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b839b397-086e-4826-a97b-483da1361524",
   "metadata": {},
   "source": [
    "### üìù Model Evaluation Results\r\n",
    "\r\n",
    "After conducting the testing phase, the model's performance yielded the following results:\r\n",
    "\r\n",
    "#### üîç Accuracy\r\n",
    "- **Overall Accuracy**: 94.8%\r\n",
    "  \r\n",
    "This indicates that the model is performing well, correctly classifying the vast majority of the digits in the MNIST test set.\r\n",
    "\r\n",
    "#### üìâ Misclassification Analysis\r\n",
    "The breakdown of misclassified digits reveals specific areas where the model struggles:\r\n",
    "\r\n",
    "- **Digit 0**: 18 misclassifications\r\n",
    "- **Digit 1**: 19 misclassifications\r\n",
    "- **Digit 2**: 30 misclassifications\r\n",
    "- **Digit 3**: 81 misclassifications\r\n",
    "- **Digit 4**: 42 misclassifications\r\n",
    "- **Digit 5**: 11 misclassifications\r\n",
    "- **Digit 6**: 60 misclassifications\r\n",
    "- **Digit 7**: 30 misclassifications\r\n",
    "- **Digit 8**: 150 misclassifications\r\n",
    "- **Digit 9**: 81 misclassifications\r\n",
    "\r\n",
    "#### üìä Insights\r\n",
    "From the analysis, we can see that the model struggles significantly with the digits **3**, **6**, **8**, and **9**, each having over **50** misclassifications. This suggests that further fine-tuning is necessary to improve the model's performance on these specific digits.\r\n",
    "\r\n",
    "### üéØ Next Steps\r\n",
    "To enhance the model's ability to classify these challenging digits, we will focus on fine-tuning the model. This adjustment aims to address the misclassification issues and improve overall performance, particularly in a real-world application where accuracy is critical.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81048bce-8815-4431-b1dd-86c3e4dddcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n",
      "Layer 2: W: torch.Size([1500, 1000]) + B: torch.Size([1500])\n",
      "Layer 3: W: torch.Size([1300, 1500]) + B: torch.Size([1300])\n",
      "Layer 4: W: torch.Size([10, 1300]) + B: torch.Size([10])\n",
      "Total number of parameters: 4,250,810\n"
     ]
    }
   ],
   "source": [
    "# Print the size of the weights matrices of the network\n",
    "# Save the count of the total number of parameters\n",
    "total_parameters_original = 0\n",
    "for index, layer in enumerate([model.linear1, model.linear2, model.linear3,model.linear4]):\n",
    "    total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n",
    "print(f'Total number of parameters: {total_parameters_original:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7570b3-4e5b-4eea-9d19-8524cd1b4d68",
   "metadata": {},
   "source": [
    "#### üìä Total Parameter Count\n",
    "The total number of parameters in the original artificial neural network is **4,250,810**.\n",
    "\n",
    "### üéØ Implications\n",
    "This substantial number of parameters indicates the model's capacity for learning complex patterns, though it may also pose challenges such as overfitting. Understanding this count is crucial as we proceed to implement LoRA to introduce additional parameters while aiming to maintain or improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7484e85-9141-4cdb-a463-9226a9f805fc",
   "metadata": {},
   "source": [
    "### üìà LoRA Parameterization Class\r\n",
    "\r\n",
    "The **LoRAParametrization** class implements the Low-Rank Adaptation (LoRA) method, enhancing our neural network's adaptability while keeping the number of trainable parameters in check. Below are the key components and functionalities:\r\n",
    "\r\n",
    "#### üõ†Ô∏è Initialization\r\n",
    "- **Parameters**:\r\n",
    "  - **lora_A**: Initialized with random Gaussian values, representing one part of the low-rank adaptation.\r\n",
    "  - **lora_B**: Initialized to zero, contributing to the learning mechanism of the network.\r\n",
    "- **Scaling Factor**: \r\n",
    "  - The scaling factor is determined as $$ \\frac{\\alpha}{r} $$, where \\( \\alpha \\) is a constant and \\( r \\) is the rank. This aids in stabilizing the training process by reducing the need for extensive hyperparameter tuning.\r\n",
    "\r\n",
    "#### üîÑ Forward Method\r\n",
    "- The **forward** method computes the adapted weights as follows:\r\n",
    "  - It adds the original weights to the product of \\( B \\) and \\( A \\), scaled appropriately.\r\n",
    "  - When the adaptation is disabled, it simply returns the original weights, allowing for flexibility during training.\r\n",
    "\r\n",
    "### üåü Purpose\r\n",
    "The LoRA function's design allows the network to learn additional representations efficiently, facilitating multi-task learning and improving performance on specific tasks without overwhelming the model with a vast number of parameters. This adaptability is particularly beneficial for fine-tuning on selected digits in the MNIST dataset.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7bfabd5-9f14-4598-a334-3f9aae9b1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParametrization(nn.Module):\n",
    "    def __init__(self, features_in, features_out, rank=1, alpha=1, device=device):\n",
    "        super().__init__()\n",
    "        # Section 4.1 of the paper: \n",
    "        #   We use a random Gaussian initialization for A and zero for B, so ‚àÜW = BA is zero at the beginning of training\n",
    "        self.lora_A = nn.Parameter(torch.zeros((rank,features_out)).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros((features_in, rank)).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "        \n",
    "        # Section 4.1 of the paper: \n",
    "        #   We then scale ‚àÜWx by Œ±/r , where Œ± is a constant in r. \n",
    "        #   When optimizing with Adam, tuning Œ± is roughly the same as tuning the learning rate if we scale the initialization appropriately. \n",
    "        #   As a result, we simply set Œ± to the first r we try and do not tune it. \n",
    "        #   This scaling helps to reduce the need to retune hyperparameters when we vary r.\n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            # Return W + (B*A)*scale\n",
    "            return original_weights + torch.matmul(self.lora_B, self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6b8d7-3a67-4cd8-a326-68a296cc1487",
   "metadata": {},
   "source": [
    "### üéØ Purpose of the LoRA Parameterization Code\r\n",
    "\r\n",
    "The provided code implements the Low-Rank Adaptation (LoRA) technique within a neural network model by applying parameterization to the weight matrices of the linear layers. This approach is specifically designed to enhance the model's adaptability and efficiency during fine-tuning for downstream tasks. Below are the key objectives of this implementation:\r\n",
    "\r\n",
    "1. **Parameterization of Linear Layers**:\r\n",
    "   - The code defines a function `linear_layer_parameterization` that only applies LoRA to the weight matrices of the linear layers, intentionally ignoring the biases. This selective adaptation is rooted in the observation that adapting only the attention weights can simplify the training process while maintaining parameter efficiency.\r\n",
    "\r\n",
    "2. **Registration of Parameterizations**:\r\n",
    "   - Each linear layer (`linear1`, `linear2`, `linear3`, `linear4`) in the model is registered for LoRA parameterization. This enables the model to utilize the benefits of LoRA, allowing it to adapt its weights effectively while minimizing the number of additional parameters introduced during training.\r\n",
    "\r\n",
    "3. **Toggle LoRA Adaptation**:\r\n",
    "   - The function `enable_disable_lora` allows for easy enabling or disabling of the LoRA adaptation across all specified layers. This feature facilitates experimentation, enabling researchers to compare the performance of the model with and without LoRA.\r\n",
    "\r\n",
    "4. **Focus on Efficiency**:\r\n",
    "   - By limiting the adaptation to the attention weights and freezing the MLP modules, the implementation aligns with the goal of reducing computational overhead and simplifying the model's architecture. This efficiency is especially beneficial in scenarios where training resources are limited or where rapid adaptation to new tasks is required.\r\n",
    "\r\n",
    "In summary, this code serves to implement a flexible, efficient mechanism for adapting neural network weights, optimizing performance for specific tasks without the need for retraining the entire model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e765e841-9d5a-44d2-9bc3-c002231bebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.parametrize as parametrize\n",
    "\n",
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    # Only add the parameterization to the weight matrix, ignore the Bias\n",
    "\n",
    "    # From section 4.2 of the paper:\n",
    "    #   We limit our study to only adapting the attention weights for downstream tasks and freeze the MLP modules (so they are not trained in downstream tasks) both for simplicity and parameter-efficiency.\n",
    "    #   [...]\n",
    "    #   We leave the empirical investigation of [...], and biases to a future work.\n",
    "    \n",
    "    features_in, features_out = layer.weight.shape\n",
    "    return LoRAParametrization(\n",
    "        features_in, features_out,\n",
    "        rank=rank, \n",
    "        alpha=lora_alpha, \n",
    "        device=device\n",
    "    )\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.linear1, \n",
    "    \"weight\", \n",
    "    linear_layer_parameterization(model.linear1, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear2, \n",
    "    \"weight\", \n",
    "    linear_layer_parameterization(model.linear2, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear3, \n",
    "    \"weight\", \n",
    "    linear_layer_parameterization(model.linear3, device)\n",
    ")\n",
    "parametrize.register_parametrization(\n",
    "    model.linear4, \n",
    "    \"weight\", \n",
    "    linear_layer_parameterization(model.linear4, device)\n",
    ")\n",
    "\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [model.linear1, model.linear2, model.linear3, model.linear4]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e0ac0-04ce-47e1-9a62-20578ef59368",
   "metadata": {},
   "source": [
    "### Number of parameters add by LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4992295b-fb9c-436e-b859-da146ea7aefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n",
      "Layer 2: W: torch.Size([1500, 1000]) + B: torch.Size([1500]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([1500, 1])\n",
      "Layer 3: W: torch.Size([1300, 1500]) + B: torch.Size([1300]) + Lora_A: torch.Size([1, 1500]) + Lora_B: torch.Size([1300, 1])\n",
      "Layer 4: W: torch.Size([10, 1300]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 1300]) + Lora_B: torch.Size([10, 1])\n",
      "Total number of parameters (original): 4,250,810\n",
      "Total number of parameters (original + LoRA): 4,259,204\n",
      "Parameters introduced by LoRA: 8,394\n",
      "Parameters incremment: 0.197%\n"
     ]
    }
   ],
   "source": [
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "for index, layer in enumerate([model.linear1, model.linear2, model.linear3,model.linear4]):\n",
    "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n",
    "    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n",
    "    print(\n",
    "        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n",
    "    )\n",
    "# The non-LoRA parameters count must match the original network\n",
    "assert total_parameters_non_lora == total_parameters_original\n",
    "print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n",
    "print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n",
    "print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n",
    "parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n",
    "print(f'Parameters incremment: {parameters_incremment:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7c081-8261-4073-b42f-2441dc9f54cb",
   "metadata": {},
   "source": [
    "### üìä Number of Parameters Added by LoRA\n",
    "\n",
    "This section evaluates the number of parameters introduced by the Low-Rank Adaptation (LoRA) technique in the neural network model. By comparing the parameters before and after the application of LoRA, we can assess its impact on the model's complexity and efficiency. \n",
    "\n",
    "#### Key Objectives:\n",
    "\n",
    "1. **Parameter Counting**:\n",
    "   - The code computes the total number of parameters added by LoRA (`Lora_A` and `Lora_B`) for each linear layer in the model.\n",
    "   - It also counts the original parameters of the model, including both weights and biases.\n",
    "\n",
    "2. **Comparison with Original Model**:\n",
    "   - An assertion checks that the count of non-LoRA parameters matches the original parameter count of the network, ensuring that no discrepancies arise from the parameterization process.\n",
    "\n",
    "3. **Results**:\n",
    "   - The total number of parameters in the original model is 4,250,810.\n",
    "   - After applying LoRA, the total parameter count increases to 4,259,204, indicating that the LoRA technique introduces an additional 8,394 parameters.\n",
    "   - This represents a parameter increment of approximately 0.197%, suggesting that the addition of LoRA parameters is minimal compared to the original model size, highlighting its efficiency in augmenting the model without significant overhead.\n",
    "\n",
    "This analysis emphasizes the effectiveness of LoRA in enhancing model adaptability while maintaining a relatively low increase in parameter count, thereby preserving computational efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e768ab-33f0-462d-bb9c-de571c24e519",
   "metadata": {},
   "source": [
    "### üîí Freezing Original Parameters for Fine-Tuning with LoRA\r\n",
    "\r\n",
    "In this section, we implement a strategy to enhance the model's performance on specific target digits (3, 6, 8, 9) from the MNIST dataset by freezing the original model parameters while allowing only the LoRA parameters to be trained.\r\n",
    "\r\n",
    "#### Key Steps:\r\n",
    "\r\n",
    "1. **Freezing Non-LoRA Parameters**:\r\n",
    "   - All parameters in the model that are not part of the LoRA parameterization are frozen, meaning they will not be updated during the training process.\r\n",
    "   - This approach preserves the original learned features of the model while allowing for adaptation through LoRA.\r\n",
    "\r\n",
    "2. **Loading the MNIST Dataset**:\r\n",
    "   - The MNIST dataset is loaded, filtered to include only the target digits specified (3, 6, 8, 9).\r\n",
    "   - A data loader is created to facilitate batch training, with a batch size of 10 and shuffling enabled.\r\n",
    "\r\n",
    "3. **Training Configuration**:\r\n",
    "   - The model is trained exclusively on the digit 9 for a limited number of iterations (100 batches) to test the effectiveness of LoRA in improving model performance on this specific digit.\r\n",
    "\r\n",
    "#### Results:\r\n",
    "- The training process is significantly faster, demonstrating the efficiency of fine-tuning only the LoRA parameters while the original parameters remain fixed. This results in reduced computation time and resources, making it an effective approach for targeted improvements in model performance.\r\n",
    "\r\n",
    "This methodology illustrates how freezing certain parameters can optimize the training process, particularly in scenarios where quick adaptations are desired without overhauling the entire model.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2fd7872-73cb-4ff5-b6f1-491f96218e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-LoRA parameter linear1.bias\n",
      "Freezing non-LoRA parameter linear1.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear2.bias\n",
      "Freezing non-LoRA parameter linear2.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear3.bias\n",
      "Freezing non-LoRA parameter linear3.parametrizations.weight.original\n",
      "Freezing non-LoRA parameter linear4.bias\n",
      "Freezing non-LoRA parameter linear4.parametrizations.weight.original\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 99/100 [00:02<00:00, 36.22it/s, loss=0.182]\n"
     ]
    }
   ],
   "source": [
    "# Freeze the non-Lora parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f'Freezing non-LoRA parameter {name}')\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Load the MNIST dataset again\n",
    "\n",
    "target_digits = [3,6,8,9]\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "include_indices = torch.tensor([target in target_digits for target in mnist_trainset.targets])\n",
    "mnist_trainset.data = mnist_trainset.data[include_indices]\n",
    "mnist_trainset.targets = mnist_trainset.targets[include_indices]\n",
    "# Create a dataloader for the training\n",
    "train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Train the network with LoRA only on the digit 9 and only for 100 batches (hoping that it would improve the performance on the digit 9)\n",
    "train(train_loader, model, epochs=1, total_iterations_limit=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689d2e1-8d6e-400a-b908-0245866f25b3",
   "metadata": {},
   "source": [
    "### Checking that the origianl weights are frozen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0b96e02-d3c1-4ba2-8ec4-2429d58262e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m enable_disable_lora(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# The original weights have been moved to net.linear1.parametrizations.weight.original\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mequal(model\u001b[38;5;241m.\u001b[39mlinear1\u001b[38;5;241m.\u001b[39mweight, model\u001b[38;5;241m.\u001b[39mlinear1\u001b[38;5;241m.\u001b[39mparametrizations\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39moriginal \u001b[38;5;241m+\u001b[39m (model\u001b[38;5;241m.\u001b[39mlinear1\u001b[38;5;241m.\u001b[39mparametrizations\u001b[38;5;241m.\u001b[39mweight[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlora_B \u001b[38;5;241m@\u001b[39m model\u001b[38;5;241m.\u001b[39mlinear1\u001b[38;5;241m.\u001b[39mparametrizations\u001b[38;5;241m.\u001b[39mweight[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlora_A) \u001b[38;5;241m*\u001b[39m model\u001b[38;5;241m.\u001b[39mlinear1\u001b[38;5;241m.\u001b[39mparametrizations\u001b[38;5;241m.\u001b[39mweight[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mscale)\n\u001b[0;32m     13\u001b[0m enable_disable_lora(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# If we disable LoRA, the linear1.weight is the original one\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check that the frozen parameters are still unchanged by the finetuning\n",
    "assert torch.all(model.linear1.parametrizations.weight.original == o_weights['linear1.weight'])\n",
    "assert torch.all(model.linear2.parametrizations.weight.original == o_weights['linear2.weight'])\n",
    "assert torch.all(model.linear3.parametrizations.weight.original == o_weights['linear3.weight'])\n",
    "assert torch.all(model.linear4.parametrizations.weight.original == o_weights['linear4.weight'])\n",
    "\n",
    "enable_disable_lora(enabled=True)\n",
    "# The new linear1.weight is obtained by the \"forward\" function of our LoRA parametrization\n",
    "# The original weights have been moved to net.linear1.parametrizations.weight.original\n",
    "# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\n",
    "assert torch.equal(model.linear1.weight, model.linear1.parametrizations.weight.original + (model.linear1.parametrizations.weight[0].lora_B @ model.linear1.parametrizations.weight[0].lora_A) * model.linear1.parametrizations.weight[0].scale)\n",
    "\n",
    "enable_disable_lora(enabled=False)\n",
    "# If we disable LoRA, the linear1.weight is the original one\n",
    "assert torch.equal(model.linear1.weight, o_weights['linear1.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8314f0-5453-4707-83eb-e8778d6c491e",
   "metadata": {},
   "source": [
    "### Testing the network with LoRA and Original weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a946a71d-9597-4f59-9d75-7e52bc7bd550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:10<00:00, 93.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956\n",
      "wrong counts for the digit 0: 17\n",
      "wrong counts for the digit 1: 30\n",
      "wrong counts for the digit 2: 40\n",
      "wrong counts for the digit 3: 24\n",
      "wrong counts for the digit 4: 52\n",
      "wrong counts for the digit 5: 78\n",
      "wrong counts for the digit 6: 26\n",
      "wrong counts for the digit 7: 85\n",
      "wrong counts for the digit 8: 39\n",
      "wrong counts for the digit 9: 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a36379d-c252-4fa0-b598-1d5e4424008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:05<00:00, 182.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.948\n",
      "wrong counts for the digit 0: 18\n",
      "wrong counts for the digit 1: 19\n",
      "wrong counts for the digit 2: 30\n",
      "wrong counts for the digit 3: 81\n",
      "wrong counts for the digit 4: 42\n",
      "wrong counts for the digit 5: 11\n",
      "wrong counts for the digit 6: 60\n",
      "wrong counts for the digit 7: 30\n",
      "wrong counts for the digit 8: 150\n",
      "wrong counts for the digit 9: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b5ea45-da58-4e9f-bc9e-7e1391ecb7d6",
   "metadata": {},
   "source": [
    "### üß™ Testing the Network with LoRA and Original Weights\r\n",
    "\r\n",
    "In this section, we evaluate the performance of the network using both LoRA-parameterized weights and the original weights to compare their effectiveness in classifying the MNIST digits.\r\n",
    "\r\n",
    "#### Testing Procedure\r\n",
    "\r\n",
    "1. **Enabling LoRA Parameters**:\r\n",
    "   - The LoRA parameters are enabled for testing to assess the impact of the adaptations made during training.\r\n",
    "\r\n",
    "2. **Testing with LoRA**:\r\n",
    "   - The model is tested on a set of 1,000 samples, and the accuracy is calculated along with the count of misclassifications for each digit.\r\n",
    "   - Initial results show a high accuracy of **95.6%** with specific wrong counts for each digit.\r\n",
    "\r\n",
    "3. **Disabling LoRA Parameters**:\r\n",
    "   - After the initial test, the LoRA parameters are disabled, and the model is tested again using the original weights.\r\n",
    "\r\n",
    "4. **Testing with Original Weights**:\r\n",
    "   - The model is evaluated again on the same set of samples, resulting in an accuracy of **94.8%** and a different distribution of wrong counts across the digits.\r\n",
    "\r\n",
    "#### Results Summary\r\n",
    "\r\n",
    "- **LoRA Enabled**:\r\n",
    "  - **Accuracy**: 95.6%\r\n",
    "  - Misclassifications are observed across different digits, with notable errors on digits such as 5 and 7.\r\n",
    "\r\n",
    "- **LoRA Disabled** (Original Weights):\r\n",
    "  - **Accuracy**: 94.8%\r\n",
    "  - The misclassification counts reveal different strengths and weaknesses compared to the LoRA-enabled model, particularly with digits like 8 and 3.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "The introduction of LoRA parameters leads to an overall improvement in accuracy compared to using only the original weights. This suggests that fine-tuning with LoRA can enhance model performance, particularly for targeted classes. Such adaptations can be crucial in scenarios requiring focused improvements in specific digit recognition tasks.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cea63a-73fc-442f-a0ce-ccb26828ab13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0c930-e881-4643-9a1d-8abf0bdbfcb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405d894-de45-49b9-bb2e-573083d3943c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d91919-1264-42c5-9e16-1cb910d20948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c6ab6-62ae-44c4-80ee-581ab164d2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
